#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Market Demand Radar V2

A market demand analysis tool implemented according to PRD v2 requirements, used to discover and evaluate unfulfilled digital product needs.
Main features:
1. Asynchronously fetch data from multiple sources
2. Use GPT-3.5-turbo to analyze and extract opportunities
3. Calculate DemandÃ—Supply scoring
4. Identify golden zone opportunities
5. Generate reports and display through Dashboard
"""

import os
import sys
import asyncio
import argparse
from datetime import datetime
from typing import Dict, Any, List

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from src.scrapers.reddit_scraper import RedditScraper, DEFAULT_SUBREDDITS
from src.scrapers.producthunt_scraper import ProductHuntScraper
from src.scrapers.appstore_scraper import AppStoreScraper
from src.scrapers.chromestore_scraper import ChromeStoreScraper
from src.extractor import LLMExtractor
from src.scoring import ScoringEngine
from src.competitive import CompetitiveFetcher
from src.report import ReportBuilder

# Set up command line arguments
def parse_args():
    parser = argparse.ArgumentParser(description="Market Demand Radar V2 - Discover unfulfilled digital product needs")
    parser.add_argument("-s", "--subreddits", nargs="+", help="List of subreddits to search")
    parser.add_argument("-l", "--limit", type=int, default=25, help="Limit of posts to search from each data source")
    parser.add_argument("-o", "--output", help="Output report filename")
    parser.add_argument("--no-gpt", action="store_true", help="Skip GPT analysis")
    parser.add_argument("--no-competitive", action="store_true", help="Skip competitive analysis")
    parser.add_argument("--gold-only", action="store_true", help="Only process golden zone opportunities")
    return parser.parse_args()

# Main function
async def main():
    args = parse_args()
    
    # Use command line arguments or default values
    subreddits = args.subreddits or DEFAULT_SUBREDDITS
    post_limit = args.limit
    output_file = args.output
    skip_gpt = args.no_gpt
    skip_competitive = args.no_competitive
    gold_only = args.gold_only
    
    print(f"ğŸ” Market Demand Radar V2 starting...")
    print(f"ğŸ“± Will fetch data from {len(subreddits)} subreddits, with a limit of {post_limit} posts each")
    
    # Collect raw posts
    raw_posts = []
    
    # 1. Fetch data from Reddit
    print("ğŸ”„ Fetching data from Reddit...")
    async with RedditScraper() as reddit_scraper:
        for subreddit_name in subreddits:
            try:
                print(f"  ğŸ“¥ Scraping r/{subreddit_name}...")
                posts = await reddit_scraper.fetch_subreddit_posts(subreddit_name, limit=post_limit)
                print(f"  âœ… Retrieved {len(posts)} posts from r/{subreddit_name}")
                raw_posts.extend(posts)
            except Exception as e:
                print(f"  âŒ Error scraping r/{subreddit_name}: {str(e)}")
    
    # 2. Fetch data from Product Hunt
    print("ğŸ”„ Fetching data from Product Hunt...")
    try:
        async with ProductHuntScraper() as ph_scraper:
            ph_posts = await ph_scraper.fetch_asks(limit=post_limit)
            print(f"  âœ… Retrieved {len(ph_posts)} posts from Product Hunt")
            raw_posts.extend(ph_posts)
    except Exception as e:
        print(f"  âŒ Error scraping Product Hunt: {str(e)}")
    
    # 3. Fetch review data from App Store
    print("ğŸ”„ Fetching review data from App Store...")
    try:
        async with AppStoreScraper() as app_scraper:
            # Configure the list of app IDs to scrape
            app_ids = ["1232780281", "310633997", "1274495053"]  # Examples: Notion, Evernote, Things 3
            for app_id in app_ids:
                app_reviews = await app_scraper.fetch_app_reviews(app_id, limit=post_limit//len(app_ids))
                print(f"  âœ… Retrieved {len(app_reviews)} reviews from App ID {app_id}")
                raw_posts.extend(app_reviews)
    except Exception as e:
        print(f"  âŒ Error scraping App Store reviews: {str(e)}")
    
    # 4. Fetch data from Chrome Web Store
    print("ğŸ”„ Fetching data from Chrome Web Store...")
    try:
        async with ChromeStoreScraper() as chrome_scraper:
            # Get popular extensions
            extensions = await chrome_scraper.fetch_top_extensions(limit=10)
            print(f"  âœ… Retrieved {len(extensions)} popular extensions")
            
            # Get extension reviews
            for extension in extensions[:5]:  # Only process the top 5 extensions
                extension_id = extension["id"]
                extension_post = await chrome_scraper.fetch_and_convert_to_raw_post(extension_id)
                raw_posts.append(extension_post)
    except Exception as e:
        print(f"  âŒ Error scraping Chrome Web Store data: {str(e)}")
    
    if not raw_posts:
        print("âŒ No posts found, please check data source configuration")
        return
    
    print(f"ğŸ“Š Collected a total of {len(raw_posts)} raw posts, starting processing...")
    
    # 5. Use LLM to extract opportunity information
    if not skip_gpt:
        print("ğŸ§  Using GPT-3.5-turbo to analyze and extract opportunity information...")
        extractor = LLMExtractor()
        processed_posts = await extractor.batch_extract(raw_posts)
        print(f"  âœ… Successfully analyzed {len(processed_posts)} posts")
    else:
        print("â© Skipping GPT analysis")
        processed_posts = raw_posts
    
    # 6. Add competitive data
    if not skip_competitive:
        print("ğŸ” Fetching competitive data...")
        fetcher = CompetitiveFetcher()
        enriched_posts = await fetcher.batch_enrich_posts(processed_posts)
        print(f"  âœ… Successfully added competitive data")
    else:
        print("â© Skipping competitive analysis")
        enriched_posts = processed_posts
    
    # 7. Calculate scores
    print("ğŸ§® Calculating DemandÃ—Supply scores...")
    scorer = ScoringEngine()
    scored_posts = scorer.score_posts(enriched_posts)
    print(f"  âœ… Successfully calculated scores")
    
    # 8. Filter golden zone opportunities
    gold_zone_posts = scorer.get_gold_zone_posts(scored_posts)
    print(f"  ğŸ¥‡ Discovered {len(gold_zone_posts)} golden zone opportunities")
    
    # 9. Generate report
    print("ğŸ“ Generating report...")
    builder = ReportBuilder()
    
    if not output_file:
        today = datetime.now().strftime("%Y-%m-%d")
        output_file = f"market_report_{today}.md"
    
    # Choose which posts to include in the report based on parameters
    report_posts = gold_zone_posts if gold_only else scored_posts
    
    report_path = builder.generate_report(report_posts, output_file)
    print(f"  âœ… Report generated: {report_path}")
    
    print("âœ¨ Processing complete!")
    print(f"ğŸ† Number of golden zone opportunities: {len(gold_zone_posts)}")
    print(f"ğŸ“Š Report path: {report_path}")

if __name__ == "__main__":
    asyncio.run(main())