# idea_radar.py (Fixed for openai>=1.0.0 API)

import praw
from datetime import datetime
import os
import openai

# === Reddit Credentials ===
REDDIT_CLIENT_ID = "VUQT-RvwOD3W-6s0R3qxGw"
REDDIT_CLIENT_SECRET = "5TW22rlhHzR9VUl7jBnKcoXoIYPhlg"
REDDIT_USER_AGENT = "ideaRadar/0.1 by Repeat_Admirable"
REDDIT_USERNAME = "Repeat_Admirable"
REDDIT_PASSWORD = "RedditPass@01"

# === OpenAI Key ===
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    raise ValueError("âŒ OPENAI_API_KEY is not set. Please export it in your shell or hardcode for local test.")

openai.api_key = OPENAI_API_KEY

# === Search Scope ===
KEYWORDS = [
    "i wish",
    "is there an app",
    "looking for an app",
    "need a chrome extension",
    "frustrating",
    "missing feature",
    "why isn't there"
]
SUBREDDITS = ["macapps", "iphone", "chrome_extensions"]

print("ğŸ” Authenticating Reddit account...")
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent=REDDIT_USER_AGENT,
    username=REDDIT_USERNAME,
    password=REDDIT_PASSWORD
)

try:
    me = reddit.user.me()
    print(f"âœ… Authenticated as: u/{me}")
except Exception as e:
    print("âŒ Reddit authentication failed:", e)
    exit(1)

def analyze_post_with_gpt(title, subreddit, url):
    prompt = f"""
Reddit Post Title:
"{title}"
Subreddit: {subreddit}
URL: {url}

Please analyze the post. Return JSON with the following fields:
- unmet_need: true/false
- pain_summary: short summary of the pain point
- alternatives: are there known tools solving this?
- solo_doable: true/false â€” can a solo dev build this?
- monetizable: true/false â€” would users likely pay?
- tags: a few keywords like 'macOS', 'productivity', 'calendar', etc.
"""
    try:
        # ä½¿ç”¨æ–°ç‰ˆOpenAI APIæ ¼å¼
        client = openai.OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âš ï¸ GPTåˆ†æå¤±è´¥: {str(e)}")
        return None

def save_to_markdown(ideas, filename=None, competitor_data=None):
    # æ–°å¢ä»·å€¼è¯„ä¼°ç»´åº¦è¯´æ˜
    VALUE_MATRIX_DESC = """
## å•†ä¸šä»·å€¼è¯„ä¼°çŸ©é˜µ
- ğŸ›  æŠ€æœ¯å¯è¡Œæ€§ï¼šç»¼åˆå¼€å‘æŠ€èƒ½åŒ¹é…åº¦ã€APIä¾èµ–å¤æ‚åº¦
- ğŸŒ å¸‚åœºé¥±å’Œåº¦ï¼šè€ƒé‡ç«å“æ•°é‡ã€å¸‚åœºå¢é•¿ç‡
- ğŸ’° å˜ç°æ½œåŠ›ï¼šLTVä¸è·å®¢æˆæœ¬å·®å€¼"""
    if not filename:
        today = datetime.today().strftime("%Y-%m-%d")
        filename = f"top_ideas_{today}.md"

    with open(filename, "w", encoding="utf-8") as f:
        f.write("# ğŸ“Œ Top Reddit App/Extension Ideas for Solo Dev\n\n")
        for idx, idea in enumerate(ideas, 1):
            f.write(f"{idx}. [{idea['title']}]({idea['url']})  ğŸ‘ {idea['score']} points\n")
            f.write(f"    Subreddit: r/{idea['subreddit']} | Posted on: {idea['created_str']}\n\n")
            if idea.get("gpt_analysis"):
                f.write("**GPT Analysis**\n")
                f.write(f"```json\n{idea['gpt_analysis']}\n```\n")
                f.write(f"**å•†ä¸šè¯„ä¼°**: {idea.get('value_insight', 'å¾…åˆ†æ')}\n\n")
            else:
                f.write("_âŒ GPT analysis failed_\n\n")

    print(f"âœ… Results saved to: {filename}")
    
    # æ·»åŠ ç«å“å¯¹æ¯”é™„å½•
    if competitor_data:
        with open(filename, "a", encoding="utf-8") as f:
            f.write("\n## ğŸ” ç«å“æµé‡å¯¹æ¯”\n")
            for domain, stats in competitor_data.items():
                f.write(f"### {domain}\n")
                if stats is not None:
                    f.write(f"- å…¨çƒæ’å: {stats.get('global_rank', 'N/A')}\n")
                    f.write(f"- å¹³å‡è®¿é—®æ—¶é•¿: {stats.get('avg_visit_duration', 0):.1f}åˆ†é’Ÿ\n")
                    f.write(f"- é¡µé¢/è®¿é—®: {stats.get('pages_per_visit', 0):.1f}\n\n")
                else:
                    f.write("- æ•°æ®è·å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç½‘ç»œè¿æ¥\n\n")

def search_ideas():
    # å¯¼å…¥ä¾èµ–æ¨¡å—
    import numpy as np
    from competitive_analysis import CompetitorAnalyzer
    from business_value import ValueAssessor
    
    results = []

    for sub in SUBREDDITS:
        subreddit = reddit.subreddit(sub)
        for keyword in KEYWORDS:
            print(f"ğŸ” Searching '{keyword}' in r/{sub}...")
            for post in subreddit.search(keyword, sort="top", time_filter="month", limit=20):
                if not post.stickied and len(post.title) < 200:
                    result = {
                        "subreddit": sub,
                        "title": post.title,
                        "score": post.score,
                        "url": post.url,
                        "created": datetime.fromtimestamp(post.created_utc),
                        "created_str": datetime.fromtimestamp(post.created_utc).strftime("%Y-%m-%d")
                    }
                    results.append(result)

    # Deduplicate
    seen = set()
    deduped = []
    for r in results:
        if r["title"] not in seen:
            seen.add(r["title"])
            deduped.append(r)

    # Sort with freshness weight
    def weighted_score(r):
        age_days = (datetime.utcnow() - r["created"]).days
        # å¢å¼ºç‰ˆç®—æ³•ï¼šæŒ‡æ•°è¡°å‡+å¯¹æ•°è½¬æ¢
        time_decay = np.exp(-age_days/45)  # 45å¤©è¡°å‡å‘¨æœŸ
        engagement_weight = np.log1p(r["score"]) * 0.8  # å¯¹æ•°è½¬æ¢é˜²åˆ·èµ
        return engagement_weight * time_decay * \
               (1 + 0.3*(r["subreddit"] in {"macapps", "chrome_extensions"}))

    # æ–°å¢æ•°æ®æ¸…æ´—
    filtered = [r for r in deduped 
               if r["score"] >= 15  # æœ€ä½ç‚¹èµé˜ˆå€¼
               and (datetime.utcnow() - r["created"]).days <= 90  # ä¸‰ä¸ªæœˆå†…
               and len(r["title"].split()) >= 5]  # æ’é™¤ç®€å•æé—®

    top = sorted(filtered, key=weighted_score, reverse=True)[:10]

    for r in top:
        print(f"\nğŸ§  GPT analyzing: {r['title']}")
        analysis = analyze_post_with_gpt(r['title'], r['subreddit'], r['url'])
        r['gpt_analysis'] = analysis

    # æ–°å¢å•†ä¸šä»·å€¼è¯„ä¼°
    assessor = ValueAssessor()
    top = assessor.enrich_with_value_analysis(top)

    # ç”Ÿæˆç«å“å¯¹æ¯”æŠ¥å‘Š
    competitor_report = {
        'similarweb': CompetitorAnalyzer('similarweb.com').get_traffic_stats(),
        'explodingtopics': CompetitorAnalyzer('explodingtopics.com').get_traffic_stats()
    }

    save_to_markdown(top, competitor_data=competitor_report)

if __name__ == "__main__":
    search_ideas()
